version 0.3: port of the relevant SNNS parts to C++, resulting in an SNNS fork named SnnsCLib.

version 0.1 and 0.2 were unpublished. 

version 0.2 used nearly unchanged SNNS code. In order to train various networks and use them later for prediction, different ways seem possible: After training, the model could be saved as a temporary .net file to disk or even to memory using e.g. the C function fmemopen. For prediction, the net could be loaded into the SNNS kernel again. Another potential possibility is to load the SNNS library various times to memory. However, all these methods have mayor drawbacks in stability, performanc, and are not parallelizable. 

version 0.1 used swig instead of Rcpp to wrap the SNNS code. This has the advantage, that the wrapper functions are automatically generated. However, it turned out that some of the interfaces of SNNS functions are quite complicated and the support of swig for R is not as advanced as for other programming languages. Extending swig in order to get the needed functionality seemed quite difficult, so that we chose to implement the wrapping manually using Rcpp.

